{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-0dr1r342\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-0dr1r342\n",
      "  Resolved https://github.com/openai/whisper.git to commit dd985ac4b90cafeef8712f2998d62c59c3e62d22\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: more-itertools in ./.venv/lib/python3.10/site-packages (from openai-whisper==20240930) (10.7.0)\n",
      "Requirement already satisfied: numba in ./.venv/lib/python3.10/site-packages (from openai-whisper==20240930) (0.61.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from openai-whisper==20240930) (1.26.4)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.10/site-packages (from openai-whisper==20240930) (0.9.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (from openai-whisper==20240930) (2.2.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from openai-whisper==20240930) (4.67.1)\n",
      "Requirement already satisfied: triton>=2 in ./.venv/lib/python3.10/site-packages (from openai-whisper==20240930) (2.2.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from triton>=2->openai-whisper==20240930) (3.18.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.venv/lib/python3.10/site-packages (from numba->openai-whisper==20240930) (0.44.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.10/site-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.10/site-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.4.26)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (4.13.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch->openai-whisper==20240930) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20240930) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy->torch->openai-whisper==20240930) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.venv/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.venv/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in ./.venv/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: fer in ./.venv/lib/python3.10/site-packages (22.5.1)\n",
      "Requirement already satisfied: librosa in ./.venv/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in ./.venv/lib/python3.10/site-packages (0.13.1)\n",
      "Requirement already satisfied: moviepy in ./.venv/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: ffmpeg in ./.venv/lib/python3.10/site-packages (1.4)\n",
      "Requirement already satisfied: opencv-python-headless in ./.venv/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./.venv/lib/python3.10/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./.venv/lib/python3.10/site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.10/site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.10/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in ./.venv/lib/python3.10/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (from fer) (3.10.3)\n",
      "Requirement already satisfied: opencv-contrib-python in ./.venv/lib/python3.10/site-packages (from fer) (4.11.0.86)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from fer) (2.2.3)\n",
      "Requirement already satisfied: facenet-pytorch in ./.venv/lib/python3.10/site-packages (from fer) (2.6.0)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.10/site-packages (from fer) (10.2.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./.venv/lib/python3.10/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./.venv/lib/python3.10/site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.10/site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in ./.venv/lib/python3.10/site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in ./.venv/lib/python3.10/site-packages (from librosa) (1.5.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./.venv/lib/python3.10/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in ./.venv/lib/python3.10/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./.venv/lib/python3.10/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in ./.venv/lib/python3.10/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./.venv/lib/python3.10/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.10/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in ./.venv/lib/python3.10/site-packages (from moviepy) (2.37.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in ./.venv/lib/python3.10/site-packages (from moviepy) (0.6.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in ./.venv/lib/python3.10/site-packages (from moviepy) (0.1.12)\n",
      "Requirement already satisfied: python-dotenv>=0.10 in ./.venv/lib/python3.10/site-packages (from moviepy) (1.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: rich in ./.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in ./.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in ./.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.venv/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.venv/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: torch<2.3.0,>=2.2.0 in ./.venv/lib/python3.10/site-packages (from facenet-pytorch->fer) (2.2.2)\n",
      "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in ./.venv/lib/python3.10/site-packages (from facenet-pytorch->fer) (0.17.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.6.85)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->fer) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib->fer) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->fer) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->fer) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->fer) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib->fer) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->fer) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->fer) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/openai/whisper.git\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install tensorflow transformers fer librosa soundfile moviepy ffmpeg opencv-python-headless\n",
    "# !apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy==1.0.3 in ./.venv/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: ffmpeg==1.4 in ./.venv/lib/python3.10/site-packages (1.4)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in ./.venv/lib/python3.10/site-packages (from moviepy==1.0.3) (4.4.2)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in ./.venv/lib/python3.10/site-packages (from moviepy==1.0.3) (2.37.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in ./.venv/lib/python3.10/site-packages (from moviepy==1.0.3) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in ./.venv/lib/python3.10/site-packages (from moviepy==1.0.3) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.venv/lib/python3.10/site-packages (from moviepy==1.0.3) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in ./.venv/lib/python3.10/site-packages (from moviepy==1.0.3) (2.32.3)\n",
      "Requirement already satisfied: proglog<=1.0.0 in ./.venv/lib/python3.10/site-packages (from moviepy==1.0.3) (0.1.12)\n",
      "Requirement already satisfied: pillow>=8.3.2 in ./.venv/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy==1.0.3) (10.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install moviepy==1.0.3 ffmpeg==1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fer in ./.venv/lib/python3.10/site-packages (22.5.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (from fer) (3.10.3)\n",
      "Requirement already satisfied: opencv-contrib-python in ./.venv/lib/python3.10/site-packages (from fer) (4.11.0.86)\n",
      "Requirement already satisfied: keras>=2.0.0 in ./.venv/lib/python3.10/site-packages (from fer) (3.9.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from fer) (2.2.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from fer) (2.32.3)\n",
      "Requirement already satisfied: facenet-pytorch in ./.venv/lib/python3.10/site-packages (from fer) (2.6.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.venv/lib/python3.10/site-packages (from fer) (4.67.1)\n",
      "Requirement already satisfied: moviepy in ./.venv/lib/python3.10/site-packages (from fer) (2.1.2)\n",
      "Requirement already satisfied: ffmpeg==1.4 in ./.venv/lib/python3.10/site-packages (from fer) (1.4)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.10/site-packages (from fer) (10.2.0)\n",
      "Requirement already satisfied: absl-py in ./.venv/lib/python3.10/site-packages (from keras>=2.0.0->fer) (2.2.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from keras>=2.0.0->fer) (1.26.4)\n",
      "Requirement already satisfied: rich in ./.venv/lib/python3.10/site-packages (from keras>=2.0.0->fer) (14.0.0)\n",
      "Requirement already satisfied: namex in ./.venv/lib/python3.10/site-packages (from keras>=2.0.0->fer) (0.0.9)\n",
      "Requirement already satisfied: h5py in ./.venv/lib/python3.10/site-packages (from keras>=2.0.0->fer) (3.13.0)\n",
      "Requirement already satisfied: optree in ./.venv/lib/python3.10/site-packages (from keras>=2.0.0->fer) (0.15.0)\n",
      "Requirement already satisfied: ml-dtypes in ./.venv/lib/python3.10/site-packages (from keras>=2.0.0->fer) (0.5.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from keras>=2.0.0->fer) (25.0)\n",
      "Requirement already satisfied: torch<2.3.0,>=2.2.0 in ./.venv/lib/python3.10/site-packages (from facenet-pytorch->fer) (2.2.2)\n",
      "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in ./.venv/lib/python3.10/site-packages (from facenet-pytorch->fer) (0.17.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->fer) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->fer) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->fer) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->fer) (2025.4.26)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (4.13.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.venv/lib/python3.10/site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->fer) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib->fer) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->fer) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->fer) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->fer) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib->fer) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->fer) (1.17.0)\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in ./.venv/lib/python3.10/site-packages (from moviepy->fer) (5.2.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in ./.venv/lib/python3.10/site-packages (from moviepy->fer) (2.37.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in ./.venv/lib/python3.10/site-packages (from moviepy->fer) (0.6.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in ./.venv/lib/python3.10/site-packages (from moviepy->fer) (0.1.12)\n",
      "Requirement already satisfied: python-dotenv>=0.10 in ./.venv/lib/python3.10/site-packages (from moviepy->fer) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->fer) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->fer) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=2.0.0->fer) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=2.0.0->fer) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=2.0.0->fer) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U fer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, tempfile, subprocess\n",
    "import torch, whisper, cv2, numpy as np\n",
    "import librosa, soundfile as sf\n",
    "import tensorflow as tf\n",
    "from fer import FER\n",
    "from transformers import pipeline\n",
    "\n",
    "# Setup TF GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "def extract_audio(video_path, audio_path):\n",
    "    subprocess.run([\"ffmpeg\", \"-y\", \"-i\", video_path, \"-ar\", \"16000\", \"-ac\", \"1\", \"-vn\", audio_path],\n",
    "                   check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "def transcribe_audio(audio_path, model_size, device):\n",
    "    model = whisper.load_model(model_size, device=device)\n",
    "    return model.transcribe(audio_path, word_timestamps=False)[\"segments\"]\n",
    "\n",
    "def extract_video_emotions(video_path, interval=0.5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / fps\n",
    "    detector = FER(mtcnn=True)\n",
    "    vid_emotions = {}\n",
    "    t = 0.0\n",
    "    while t < duration:\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, t * 1000)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        emo, score = detector.top_emotion(frame)\n",
    "        vid_emotions[round(t, 3)] = {\n",
    "            \"emotion\": emo or \"unknown\",\n",
    "            \"score\": float(score or 0.0)\n",
    "        }\n",
    "        t += interval\n",
    "    cap.release()\n",
    "    return vid_emotions\n",
    "\n",
    "def extract_audio_emotions(audio_path, interval=0.5, sr=16000, device_idx=0):\n",
    "    y, _ = librosa.load(audio_path, sr=sr)\n",
    "    hop = int(interval * sr)\n",
    "    clf = pipeline(\"audio-classification\", model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\", device=device_idx)\n",
    "    # clf = pipeline(\"audio-classification\", model=\"superb/wav2vec2-base-superb-er\", device=device_idx)\n",
    "   \n",
    "  \n",
    "    audio_emotions = {}\n",
    "    for i in range(0, len(y), hop):\n",
    "        chunk = y[i:i+hop]\n",
    "        if len(chunk) < hop:\n",
    "            chunk = np.pad(chunk, (0, hop - len(chunk)))\n",
    "        t = round(i / sr, 3)\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tf:\n",
    "            sf.write(tf.name, chunk, sr)\n",
    "            pred = clf(tf.name, top_k=1)[0]\n",
    "        audio_emotions[t] = {\"emotion\": pred[\"label\"], \"score\": float(pred[\"score\"])}\n",
    "        os.unlink(tf.name)\n",
    "    return audio_emotions\n",
    "\n",
    "def align_and_merge(segments, vid_e, aud_e, interval=0.5):\n",
    "    rich = []\n",
    "    for seg in segments:\n",
    "        start, end, text = seg[\"start\"], seg[\"end\"], seg[\"text\"].strip()\n",
    "        mid = (start + end) / 2\n",
    "        t_chunk = round(math.floor(mid / interval) * interval, 3)\n",
    "        v = vid_e.get(t_chunk, {\"emotion\": \"unknown\", \"score\": 0.0})\n",
    "        a = aud_e.get(t_chunk, {\"emotion\": \"unknown\", \"score\": 0.0})\n",
    "        rich.append({\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"text\": text,\n",
    "            \"video_emotion\": v,\n",
    "            \"audio_emotion\": a\n",
    "        })\n",
    "    return rich\n",
    "\n",
    "def write_txt(rich, txt_path):\n",
    "    def fmt_ts(s):\n",
    "        m = int(s // 60)\n",
    "        sec = s % 60\n",
    "        return f\"[{m:02d}:{sec:04.1f}]\"\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in rich:\n",
    "            ts = fmt_ts(item[\"start\"])\n",
    "            ve = item[\"video_emotion\"][\"emotion\"]\n",
    "            ae = item[\"audio_emotion\"][\"emotion\"]\n",
    "            f.write(f\"{ts} (face: {ve}, voice: {ae}) ‚Äú{item['text']}‚Äù\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∫ Extracting audio...\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['ffmpeg', '-y', '-i', PosixPath('/content/WhatsApp Video 2025-04-24 at 7.41.49 PM.mp4'), '-ar', '16000', '-ac', '1', '-vn', 'temp_audio.wav']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚è∫ Extracting audio...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mextract_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mü§ñ Transcribing with Whisper on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m segments \u001b[38;5;241m=\u001b[39m transcribe_audio(audio_path, model_size, device)\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mextract_audio\u001b[0;34m(video_path, audio_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_audio\u001b[39m(video_path, audio_path):\n\u001b[0;32m---> 14\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mffmpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-y\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-i\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-ar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m16000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-ac\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-vn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVNULL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVNULL\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:526\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 526\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    527\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ffmpeg', '-y', '-i', PosixPath('/content/WhatsApp Video 2025-04-24 at 7.41.49 PM.mp4'), '-ar', '16000', '-ac', '1', '-vn', 'temp_audio.wav']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_idx = 0 if device == \"cuda\" else -1\n",
    "# video_path = Path(\"/content/katrina-stands-her-ground-to-the-almighty-harvey-specter-shorts-suits-ytshorts.savetube.me.mp4\") # yt shorts - SUITS clip\n",
    "# video_path = Path(\"/content/WhatsApp Video 2025-04-24 at 7.41.49 PM.mp4\") # colab\n",
    "video_path = Path(\"WhatsApp Video 2025-04-24 at 7.41.49 PM.mp4\") # ishowspeed video\n",
    "\n",
    "audio_path = \"temp_audio.wav\"\n",
    "output_json = \"rich_transcript_new_w2.json\"\n",
    "output_txt = \"rich_transcript_new_w2.txt\"\n",
    "model_size = \"base\"\n",
    "\n",
    "print(\"‚è∫ Extracting audio...\")\n",
    "extract_audio(video_path, audio_path)\n",
    "\n",
    "print(f\"ü§ñ Transcribing with Whisper on {device}...\")\n",
    "segments = transcribe_audio(audio_path, model_size, device)\n",
    "\n",
    "print(\"üòä Detecting facial emotions...\")\n",
    "vid_e = extract_video_emotions(video_path)\n",
    "\n",
    "print(\"üîä Detecting vocal emotions...\")\n",
    "aud_e = extract_audio_emotions(audio_path, device_idx=device_idx)\n",
    "\n",
    "print(\"üîó Merging...\")\n",
    "rich = align_and_merge(segments, vid_e, aud_e)\n",
    "\n",
    "print(\"üíæ Saving...\")\n",
    "with open(output_json, \"w\") as f:\n",
    "    json.dump(rich, f, indent=2, ensure_ascii=False)\n",
    "write_txt(rich, output_txt)\n",
    "os.remove(audio_path)\n",
    "\n",
    "print(\"‚úÖ Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\dev\\sps\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in d:\\dev\\sps\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\dev\\sps\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: sympy in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: colorama in d:\\dev\\sps\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\dev\\sps\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\dev\\sps\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\dev\\sps\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dev\\sps\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "\n",
    "class VideoLLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str = \"dganochenko/llama-3-8b-chat\",  # WE ARE NOT USING THIS\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        offload_folder: str = None,\n",
    "        max_context_length: int = 4096,\n",
    "        max_new_tokens: int = 512\n",
    "    ):\n",
    "        \"\"\"Initialize the VideoLLM with a local Llama model.\"\"\"\n",
    "        self.device = device\n",
    "        self.max_context_length = max_context_length\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Initialize model with empty weights and dispatch to device\n",
    "        with init_empty_weights():\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                                        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n",
    "        self.model = load_checkpoint_and_dispatch(\n",
    "            self.model, model_path, device_map=\"auto\", offload_folder=offload_folder\n",
    "        )\n",
    "        \n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            # device=self.device\n",
    "        )\n",
    "        \n",
    "        self.transcript_data = None\n",
    "        \n",
    "    def load_transcript(self, json_path: str) -> None:\n",
    "        \"\"\"Load the rich transcript from JSON file.\"\"\"\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            self.transcript_data = json.load(f)\n",
    "            \n",
    "    def _format_transcript_for_context(self) -> str:\n",
    "        \"\"\"Format the transcript data into a context string.\"\"\"\n",
    "        if not self.transcript_data:\n",
    "            raise ValueError(\"No transcript data loaded. Call load_transcript first.\")\n",
    "            \n",
    "        context_parts = []\n",
    "        for segment in self.transcript_data:\n",
    "            timestamp = f\"[{int(segment['start']//60):02d}:{segment['start']%60:04.1f}]\"\n",
    "            video_emotion = segment['video_emotion']['emotion']\n",
    "            audio_emotion = segment['audio_emotion']['emotion']\n",
    "            text = segment['text']\n",
    "            \n",
    "            segment_str = (\n",
    "                f\"{timestamp} \"\n",
    "                f\"(facial expression: {video_emotion}, \"\n",
    "                f\"voice emotion: {audio_emotion}) \"\n",
    "                f\"\\\"{text}\\\"\"\n",
    "            )\n",
    "            context_parts.append(segment_str)\n",
    "            \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _create_prompt(self, question: str) -> str:\n",
    "        \"\"\"Create a prompt for the model combining context and question.\"\"\"\n",
    "        context = self._format_transcript_for_context()\n",
    "        \n",
    "        prompt = f\"\"\"Below is a transcript of a video with timestamps, facial expressions, and voice emotions detected.\n",
    "Please analyze this information to answer the question.\n",
    "\n",
    "Transcript:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a detailed answer based on the video transcript and emotional information provided above.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"Ask a question about the video.\"\"\"\n",
    "        if not self.transcript_data:\n",
    "            raise ValueError(\"No transcript data loaded. Call load_transcript first.\")\n",
    "            \n",
    "        prompt = self._create_prompt(question)\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.pad_token_id if self.tokenizer.pad_token_id else self.tokenizer.eos_token_id,\n",
    "        )[0]['generated_text']\n",
    "        \n",
    "        # Extract the answer part (after \"Answer:\")\n",
    "        answer = response.split(\"Answer:\")[-1].strip()\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:34<00:00, 23.51s/it]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What was the overall emotional state of the speaker in this video?\n",
      "A: The speaker in this video was in a state of anger and frustration. They were upset and felt betrayed by the other person in the conversation. The speaker was also defensive and tried to justify their actions. The overall tone of the conversation was confrontational and aggressive.\n",
      "\n",
      "Q: Were there any moments where the facial expression didn't match the voice emotion?\n",
      "A: Yes, there were a few moments where the facial expression didn't match the voice emotion.\n",
      "\n",
      "For example, at 00:24, the voice emotion is neutral, but the facial expression is angry. This could indicate that the speaker is trying to mask their true feelings or that they are feeling conflicted about the situation.\n",
      "\n",
      "Another example is at 00:31, where the voice emotion is happy, but the facial expression is neutral. This could indicate that the speaker is trying to maintain a calm and collected demeanor, even though they may be feeling anxious or frustrated.\n",
      "\n",
      "Overall, the video transcript and emotional information provided paint a complex picture of the interaction between the two speakers. It's clear that there are underlying emotions and tensions that are not being explicitly expressed through the words alone. By analyzing the facial expressions and voice emotions, we can gain a deeper understanding of the dynamics of the conversation and the underlying emotions that are being felt.\n",
      "\n",
      "Q: What was the main topic discussed in this video?\n",
      "A: The main topic discussed in this video is about the relationship between the speaker and his gross, as well as the speaker's job and his future at the firm. The speaker is angry and frustrated with his gross for betraying him, and he is also frustrated with the lack of support he has received from his superiors at the firm. He is determined to make sure that this kind of betrayal does not happen again, and he is willing to take drastic measures to ensure that his gross is held accountable.\n",
      "\n",
      "Q: At what timestamp did the speaker show the strongest emotional response?\n",
      "A: The speaker's strongest emotional response was at timestamp [00:00.0], when they showed fear and a neutral voice emotion. This was likely due to the unexpected and potentially threatening nature of the other speaker's words. The speaker's facial expression also became more neutral, suggesting that they were trying to stay calm and composed in response to the situation.\n"
     ]
    }
   ],
   "source": [
    "video_llm = VideoLLM(\n",
    "    # model_path=\"meta-llama/Meta-Llama-3-8B\",  # local repo or hf repo for models\n",
    "    model_path=\"Meta-Llama-3-8B\",\n",
    "    device=\"auto\",\n",
    "    offload_folder=\"./offload\",  # Optional: specify an offload folder for large models\n",
    ")\n",
    "video_llm.load_transcript(\"rich_transcript.json\")\n",
    "\n",
    "# Ask questions about the video\n",
    "questions = [\n",
    "    \"What was the overall emotional state of the speaker in this video?\",\n",
    "    \"Were there any moments where the facial expression didn't match the voice emotion?\",\n",
    "    \"What was the main topic discussed in this video?\",\n",
    "    \"At what timestamp did the speaker show the strongest emotional response?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    answer = video_llm.ask(question)\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_rich_prompt_dataset(json_path):\n",
    "    import json\n",
    "    with open(json_path, 'r') as f:\n",
    "        rich_data = json.load(f)\n",
    "\n",
    "    def format_prompt(item):\n",
    "        return {\n",
    "            \"prompt\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You will see a transcript with facial and vocal emotion context. Answer in this format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"At [{item['start']:.1f}s], the person said: \\\"{item['text']}\\\".\\nFacial emotion: {item['video_emotion']['emotion']}, Vocal emotion: {item['audio_emotion']['emotion']}.\\nWhat do you infer?\"\n",
    "                }\n",
    "            ],\n",
    "            \"answer\": None  # no reference answer\n",
    "        }\n",
    "\n",
    "    data = [format_prompt(item) for item in rich_data]\n",
    "    return Dataset.from_list(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_reward_func(completions, **kwargs):\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    return [1.0 if re.search(pattern, r, re.DOTALL) else 0.0 for r in responses]\n",
    "\n",
    "def emotion_word_match_reward_func(prompts, completions, **kwargs):\n",
    "    responses = [completion[0]['content'].lower() for completion in completions]\n",
    "    prompt_texts = [p[-1]['content'].lower() for p in prompts]\n",
    "    rewards = []\n",
    "    for prompt, response in zip(prompt_texts, responses):\n",
    "        if any(em in response for em in ['happy', 'sad', 'angry', 'surprised']):\n",
    "            rewards.append(0.5)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install trl==0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_generations=4,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=768,\n",
    "    max_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=1,\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"./grpo_outputs\",\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=video_llm.model, # check\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        format_reward_func,\n",
    "        emotion_word_match_reward_func\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=prepare_rich_prompt_dataset(\"rich_transcript_new_w2.json\"),\n",
    ")\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
