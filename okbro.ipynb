{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\ups\\appdata\\local\\temp\\pip-req-build-az97zrc5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Error [WinError 2] The system cannot find the file specified while executing command git version\n",
      "ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\dev\\sps\\.venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in d:\\dev\\sps\\.venv\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in d:\\dev\\sps\\.venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in d:\\dev\\sps\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in d:\\dev\\sps\\.venv\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: transformers in d:\\dev\\sps\\.venv\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: fer in d:\\dev\\sps\\.venv\\lib\\site-packages (22.5.1)\n",
      "Requirement already satisfied: librosa in d:\\dev\\sps\\.venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in d:\\dev\\sps\\.venv\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: opencv-python-headless in d:\\dev\\sps\\.venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: filelock in d:\\dev\\sps\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\dev\\sps\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\dev\\sps\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\dev\\sps\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\dev\\sps\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in d:\\dev\\sps\\.venv\\lib\\site-packages (from fer) (3.10.1)\n",
      "Requirement already satisfied: opencv-contrib-python in d:\\dev\\sps\\.venv\\lib\\site-packages (from fer) (4.11.0.86)\n",
      "Requirement already satisfied: pandas in d:\\dev\\sps\\.venv\\lib\\site-packages (from fer) (2.2.3)\n",
      "Requirement already satisfied: facenet-pytorch in d:\\dev\\sps\\.venv\\lib\\site-packages (from fer) (2.6.0)\n",
      "Requirement already satisfied: moviepy in d:\\dev\\sps\\.venv\\lib\\site-packages (from fer) (2.1.2)\n",
      "Requirement already satisfied: ffmpeg==1.4 in d:\\dev\\sps\\.venv\\lib\\site-packages (from fer) (1.4)\n",
      "Requirement already satisfied: Pillow in d:\\dev\\sps\\.venv\\lib\\site-packages (from fer) (10.2.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in d:\\dev\\sps\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in d:\\dev\\sps\\.venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: pycparser in d:\\dev\\sps\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: rich in d:\\dev\\sps\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in d:\\dev\\sps\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in d:\\dev\\sps\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\dev\\sps\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\dev\\sps\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\dev\\sps\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: colorama in d:\\dev\\sps\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: torch<2.3.0,>=2.2.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from facenet-pytorch->fer) (2.2.2)\n",
      "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from facenet-pytorch->fer) (0.17.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from matplotlib->fer) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\dev\\sps\\.venv\\lib\\site-packages (from matplotlib->fer) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from matplotlib->fer) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from matplotlib->fer) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from matplotlib->fer) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\dev\\sps\\.venv\\lib\\site-packages (from matplotlib->fer) (2.9.0.post0)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in d:\\dev\\sps\\.venv\\lib\\site-packages (from moviepy->fer) (2.37.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from moviepy->fer) (0.6.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from moviepy->fer) (0.1.11)\n",
      "Requirement already satisfied: python-dotenv>=0.10 in d:\\dev\\sps\\.venv\\lib\\site-packages (from moviepy->fer) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from pandas->fer) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\dev\\sps\\.venv\\lib\\site-packages (from pandas->fer) (2025.2)\n",
      "Requirement already satisfied: sympy in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch->fer) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/openai/whisper.git\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install tensorflow transformers fer librosa soundfile opencv-python-headless\n",
    "# !apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, tempfile, subprocess\n",
    "import torch, whisper, cv2, numpy as np\n",
    "import librosa, soundfile as sf\n",
    "import tensorflow as tf\n",
    "from fer import FER\n",
    "from transformers import pipeline\n",
    "\n",
    "# Setup TF GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "def extract_audio(video_path, audio_path):\n",
    "    subprocess.run([\"ffmpeg\", \"-y\", \"-i\", video_path, \"-ar\", \"16000\", \"-ac\", \"1\", \"-vn\", audio_path],\n",
    "                   check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "def transcribe_audio(audio_path, model_size, device):\n",
    "    model = whisper.load_model(model_size, device=device)\n",
    "    return model.transcribe(audio_path, word_timestamps=False)[\"segments\"]\n",
    "\n",
    "def extract_video_emotions(video_path, interval=0.5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / fps\n",
    "    detector = FER(mtcnn=True)\n",
    "    vid_emotions = {}\n",
    "    t = 0.0\n",
    "    while t < duration:\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, t * 1000)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        emo, score = detector.top_emotion(frame)\n",
    "        vid_emotions[round(t, 3)] = {\n",
    "            \"emotion\": emo or \"unknown\",\n",
    "            \"score\": float(score or 0.0)\n",
    "        }\n",
    "        t += interval\n",
    "    cap.release()\n",
    "    return vid_emotions\n",
    "\n",
    "def extract_audio_emotions(audio_path, interval=0.5, sr=16000, device_idx=0):\n",
    "    y, _ = librosa.load(audio_path, sr=sr)\n",
    "    hop = int(interval * sr)\n",
    "    clf = pipeline(\"audio-classification\", model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\", device=device_idx)\n",
    "    # clf = pipeline(\"audio-classification\", model=\"superb/wav2vec2-base-superb-er\", device=device_idx)\n",
    "   \n",
    "  \n",
    "    audio_emotions = {}\n",
    "    for i in range(0, len(y), hop):\n",
    "        chunk = y[i:i+hop]\n",
    "        if len(chunk) < hop:\n",
    "            chunk = np.pad(chunk, (0, hop - len(chunk)))\n",
    "        t = round(i / sr, 3)\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tf:\n",
    "            sf.write(tf.name, chunk, sr)\n",
    "            pred = clf(tf.name, top_k=1)[0]\n",
    "        audio_emotions[t] = {\"emotion\": pred[\"label\"], \"score\": float(pred[\"score\"])}\n",
    "        os.unlink(tf.name)\n",
    "    return audio_emotions\n",
    "\n",
    "def align_and_merge(segments, vid_e, aud_e, interval=0.5):\n",
    "    rich = []\n",
    "    for seg in segments:\n",
    "        start, end, text = seg[\"start\"], seg[\"end\"], seg[\"text\"].strip()\n",
    "        mid = (start + end) / 2\n",
    "        t_chunk = round(math.floor(mid / interval) * interval, 3)\n",
    "        v = vid_e.get(t_chunk, {\"emotion\": \"unknown\", \"score\": 0.0})\n",
    "        a = aud_e.get(t_chunk, {\"emotion\": \"unknown\", \"score\": 0.0})\n",
    "        rich.append({\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"text\": text,\n",
    "            \"video_emotion\": v,\n",
    "            \"audio_emotion\": a\n",
    "        })\n",
    "    return rich\n",
    "\n",
    "def write_txt(rich, txt_path):\n",
    "    def fmt_ts(s):\n",
    "        m = int(s // 60)\n",
    "        sec = s % 60\n",
    "        return f\"[{m:02d}:{sec:04.1f}]\"\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in rich:\n",
    "            ts = fmt_ts(item[\"start\"])\n",
    "            ve = item[\"video_emotion\"][\"emotion\"]\n",
    "            ae = item[\"audio_emotion\"][\"emotion\"]\n",
    "            f.write(f\"{ts} (face: {ve}, voice: {ae}) â€œ{item['text']}â€\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_idx = 0 if device == \"cuda\" else -1\n",
    "# video_path = Path(\"/content/katrina-stands-her-ground-to-the-almighty-harvey-specter-shorts-suits-ytshorts.savetube.me.mp4\")\n",
    "video_path = Path(\"/content/WhatsApp Video 2025-04-24 at 7.41.49 PM.mp4\")\n",
    "\n",
    "audio_path = \"temp_audio.wav\"\n",
    "output_json = \"rich_transcript_new_w2.json\"\n",
    "output_txt = \"rich_transcript_new_w2.txt\"\n",
    "model_size = \"base\"\n",
    "\n",
    "print(\"âº Extracting audio...\")\n",
    "extract_audio(video_path, audio_path)\n",
    "\n",
    "print(f\"ðŸ¤– Transcribing with Whisper on {device}...\")\n",
    "segments = transcribe_audio(audio_path, model_size, device)\n",
    "\n",
    "print(\"ðŸ˜Š Detecting facial emotions...\")\n",
    "vid_e = extract_video_emotions(video_path)\n",
    "\n",
    "print(\"ðŸ”Š Detecting vocal emotions...\")\n",
    "aud_e = extract_audio_emotions(audio_path, device_idx=device_idx)\n",
    "\n",
    "print(\"ðŸ”— Merging...\")\n",
    "rich = align_and_merge(segments, vid_e, aud_e)\n",
    "\n",
    "print(\"ðŸ’¾ Saving...\")\n",
    "with open(output_json, \"w\") as f:\n",
    "    json.dump(rich, f, indent=2, ensure_ascii=False)\n",
    "write_txt(rich, output_txt)\n",
    "os.remove(audio_path)\n",
    "\n",
    "print(\"âœ… Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\dev\\sps\\.venv\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\dev\\sps\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in d:\\dev\\sps\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\dev\\sps\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: sympy in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\dev\\sps\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: colorama in d:\\dev\\sps\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\dev\\sps\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\dev\\sps\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\dev\\sps\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\dev\\sps\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\dev\\sps\\.venv\\lib\\site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dev\\sps\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "\n",
    "class VideoLLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str = \"dganochenko/llama-3-8b-chat\",  # Update with your local model path\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        offload_folder: str = None,\n",
    "        max_context_length: int = 4096,\n",
    "        max_new_tokens: int = 512\n",
    "    ):\n",
    "        \"\"\"Initialize the VideoLLM with a local Llama model.\"\"\"\n",
    "        self.device = device\n",
    "        self.max_context_length = max_context_length\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Initialize model with empty weights and dispatch to device\n",
    "        with init_empty_weights():\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                                        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n",
    "        self.model = load_checkpoint_and_dispatch(\n",
    "            self.model, model_path, device_map=\"auto\", offload_folder=offload_folder\n",
    "        )\n",
    "        \n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            # device=self.device\n",
    "        )\n",
    "        \n",
    "        self.transcript_data = None\n",
    "        \n",
    "    def load_transcript(self, json_path: str) -> None:\n",
    "        \"\"\"Load the rich transcript from JSON file.\"\"\"\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            self.transcript_data = json.load(f)\n",
    "            \n",
    "    def _format_transcript_for_context(self) -> str:\n",
    "        \"\"\"Format the transcript data into a context string.\"\"\"\n",
    "        if not self.transcript_data:\n",
    "            raise ValueError(\"No transcript data loaded. Call load_transcript first.\")\n",
    "            \n",
    "        context_parts = []\n",
    "        for segment in self.transcript_data:\n",
    "            timestamp = f\"[{int(segment['start']//60):02d}:{segment['start']%60:04.1f}]\"\n",
    "            video_emotion = segment['video_emotion']['emotion']\n",
    "            audio_emotion = segment['audio_emotion']['emotion']\n",
    "            text = segment['text']\n",
    "            \n",
    "            segment_str = (\n",
    "                f\"{timestamp} \"\n",
    "                f\"(facial expression: {video_emotion}, \"\n",
    "                f\"voice emotion: {audio_emotion}) \"\n",
    "                f\"\\\"{text}\\\"\"\n",
    "            )\n",
    "            context_parts.append(segment_str)\n",
    "            \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _create_prompt(self, question: str) -> str:\n",
    "        \"\"\"Create a prompt for the model combining context and question.\"\"\"\n",
    "        context = self._format_transcript_for_context()\n",
    "        \n",
    "        prompt = f\"\"\"Below is a transcript of a video with timestamps, facial expressions, and voice emotions detected.\n",
    "Please analyze this information to answer the question.\n",
    "\n",
    "Transcript:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a detailed answer based on the video transcript and emotional information provided above.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"Ask a question about the video.\"\"\"\n",
    "        if not self.transcript_data:\n",
    "            raise ValueError(\"No transcript data loaded. Call load_transcript first.\")\n",
    "            \n",
    "        prompt = self._create_prompt(question)\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.pad_token_id if self.tokenizer.pad_token_id else self.tokenizer.eos_token_id,\n",
    "        )[0]['generated_text']\n",
    "        \n",
    "        # Extract the answer part (after \"Answer:\")\n",
    "        answer = response.split(\"Answer:\")[-1].strip()\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:34<00:00, 23.51s/it]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What was the overall emotional state of the speaker in this video?\n",
      "A: The speaker in this video was in a state of anger and frustration. They were upset and felt betrayed by the other person in the conversation. The speaker was also defensive and tried to justify their actions. The overall tone of the conversation was confrontational and aggressive.\n",
      "\n",
      "Q: Were there any moments where the facial expression didn't match the voice emotion?\n",
      "A: Yes, there were a few moments where the facial expression didn't match the voice emotion.\n",
      "\n",
      "For example, at 00:24, the voice emotion is neutral, but the facial expression is angry. This could indicate that the speaker is trying to mask their true feelings or that they are feeling conflicted about the situation.\n",
      "\n",
      "Another example is at 00:31, where the voice emotion is happy, but the facial expression is neutral. This could indicate that the speaker is trying to maintain a calm and collected demeanor, even though they may be feeling anxious or frustrated.\n",
      "\n",
      "Overall, the video transcript and emotional information provided paint a complex picture of the interaction between the two speakers. It's clear that there are underlying emotions and tensions that are not being explicitly expressed through the words alone. By analyzing the facial expressions and voice emotions, we can gain a deeper understanding of the dynamics of the conversation and the underlying emotions that are being felt.\n",
      "\n",
      "Q: What was the main topic discussed in this video?\n",
      "A: The main topic discussed in this video is about the relationship between the speaker and his gross, as well as the speaker's job and his future at the firm. The speaker is angry and frustrated with his gross for betraying him, and he is also frustrated with the lack of support he has received from his superiors at the firm. He is determined to make sure that this kind of betrayal does not happen again, and he is willing to take drastic measures to ensure that his gross is held accountable.\n",
      "\n",
      "Q: At what timestamp did the speaker show the strongest emotional response?\n",
      "A: The speaker's strongest emotional response was at timestamp [00:00.0], when they showed fear and a neutral voice emotion. This was likely due to the unexpected and potentially threatening nature of the other speaker's words. The speaker's facial expression also became more neutral, suggesting that they were trying to stay calm and composed in response to the situation.\n"
     ]
    }
   ],
   "source": [
    "video_llm = VideoLLM(\n",
    "    # model_path=\"meta-llama/Meta-Llama-3-8B\",  # Update with your local model\n",
    "    model_path=\"Meta-Llama-3-8B\",\n",
    "    device=\"auto\",\n",
    "    offload_folder=\"./offload\",  # Optional: specify an offload folder for large models\n",
    ")\n",
    "video_llm.load_transcript(\"rich_transcript.json\")\n",
    "\n",
    "# Ask questions about the video\n",
    "questions = [\n",
    "    \"What was the overall emotional state of the speaker in this video?\",\n",
    "    \"Were there any moments where the facial expression didn't match the voice emotion?\",\n",
    "    \"What was the main topic discussed in this video?\",\n",
    "    \"At what timestamp did the speaker show the strongest emotional response?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    answer = video_llm.ask(question)\n",
    "    print(f\"A: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
